{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"773801ff","cell_type":"code","source":"import torch\nfrom torchvision import datasets, transforms\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom datetime import datetime\nfrom torch.utils.data import DataLoader, random_split\nfrom torchmetrics import Accuracy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T21:28:28.046786Z","iopub.execute_input":"2025-04-14T21:28:28.047084Z","iopub.status.idle":"2025-04-14T21:28:35.457011Z","shell.execute_reply.started":"2025-04-14T21:28:28.047062Z","shell.execute_reply":"2025-04-14T21:28:35.456341Z"}},"outputs":[],"execution_count":1},{"id":"f4dcb451","cell_type":"code","source":"transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor()\n])\n\n# Downloading EMNIST Letters dataset \nemnist = datasets.EMNIST(root='./data', split='letters', train=True, download=True, transform=transform)\nprint(emnist.class_to_idx)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T21:28:39.900481Z","iopub.execute_input":"2025-04-14T21:28:39.900970Z","iopub.status.idle":"2025-04-14T21:29:06.875758Z","shell.execute_reply.started":"2025-04-14T21:28:39.900941Z","shell.execute_reply":"2025-04-14T21:29:06.874742Z"}},"outputs":[{"name":"stdout","text":"Downloading https://biometrics.nist.gov/cs_links/EMNIST/gzip.zip to ./data/EMNIST/raw/gzip.zip\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 562M/562M [00:10<00:00, 52.5MB/s] \n","output_type":"stream"},{"name":"stdout","text":"Extracting ./data/EMNIST/raw/gzip.zip to ./data/EMNIST/raw\n{'N/A': 0, 'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26}\n","output_type":"stream"}],"execution_count":2},{"id":"be9a60cb","cell_type":"code","source":"total_len = len(emnist)\ntrain_size = int(0.75 * total_len)\nval_size = int(0.20 * total_len)\nextra_size = total_len - train_size - val_size\n\ntrain_dataset, val_dataset, extra_dataset = random_split(emnist, [train_size, val_size, extra_size])\n\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T21:29:15.423552Z","iopub.execute_input":"2025-04-14T21:29:15.423841Z","iopub.status.idle":"2025-04-14T21:29:15.450460Z","shell.execute_reply.started":"2025-04-14T21:29:15.423821Z","shell.execute_reply":"2025-04-14T21:29:15.449715Z"}},"outputs":[],"execution_count":3},{"id":"bead3fc7","cell_type":"code","source":"print(f\"Train set size: {len(train_dataset)}\")\nprint(f\"Validation set size: {len(val_dataset)}\")\nprint(f\"Extra set size: {len(extra_dataset)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T21:29:21.621230Z","iopub.execute_input":"2025-04-14T21:29:21.621602Z","iopub.status.idle":"2025-04-14T21:29:21.626180Z","shell.execute_reply.started":"2025-04-14T21:29:21.621562Z","shell.execute_reply":"2025-04-14T21:29:21.625120Z"}},"outputs":[{"name":"stdout","text":"Train set size: 93600\nValidation set size: 24960\nExtra set size: 6240\n","output_type":"stream"}],"execution_count":4},{"id":"5c08de59","cell_type":"code","source":"class ResBlock(nn.Module):\n    def __init__(self, input_features, output_features):\n        super(ResBlock, self).__init__()\n        self.stride = 1 if input_features == output_features else 2\n        \n        #main convolutional path\n        self.features = nn.Sequential(\n            nn.Conv2d(input_features, output_features, kernel_size=3, stride=self.stride, padding=1, bias=False),\n            nn.BatchNorm2d(output_features),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(output_features, output_features, kernel_size=3, stride=1, padding=1, bias=False),\n            nn.BatchNorm2d(output_features)\n        )\n\n        #shortcut connection\n        self.shortcut = nn.Identity()\n        if input_features != output_features:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(input_features, output_features, kernel_size=1, stride=self.stride, bias=False),\n                nn.BatchNorm2d(output_features)\n            )\n\n    def forward(self, x):\n        residual = self.shortcut(x)\n        x = self.features(x)\n        x += residual\n        x = F.relu(x, inplace=True)\n        return x\n\nclass Resnet18(nn.Module):\n    def __init__(self, num_of_classes=26):  #classes is 26 for EMNIST letters\n        super(Resnet18, self).__init__()\n        \n        self.features = nn.Sequential(\n            nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False), \n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n\n            ResBlock(64, 64),\n            ResBlock(64, 64),\n\n            ResBlock(64, 128),\n            ResBlock(128, 128),\n\n            ResBlock(128, 256),\n            ResBlock(256, 256),\n\n            ResBlock(256, 512),\n            ResBlock(512, 512),\n\n            nn.AdaptiveAvgPool2d((1, 1))\n        )\n\n        self.classifier = nn.Sequential(\n            nn.Linear(512, num_of_classes)\n        )\n\n    def forward(self, x):\n        x = self.features(x)\n        x = torch.flatten(x, 1)\n        x = self.classifier(x)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T21:29:25.019593Z","iopub.execute_input":"2025-04-14T21:29:25.019893Z","iopub.status.idle":"2025-04-14T21:29:25.029384Z","shell.execute_reply.started":"2025-04-14T21:29:25.019869Z","shell.execute_reply":"2025-04-14T21:29:25.028604Z"}},"outputs":[],"execution_count":5},{"id":"77f3f07e","cell_type":"code","source":"def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, device, model_save_path=\"best_emnist_model.pth\"):\n    model.to(device)\n    \n    best_val_acc = 0.0\n    history = {\n        \"train\": {\"losses\": [], \"accuracies\": [], \"epoch_times\": []},\n        \"val\": {\"losses\": [], \"accuracies\": []},\n        \"total_training_time\": None\n    }\n\n    total_training_start = datetime.now()\n    \n    for epoch in range(num_epochs):\n        epoch_start_time = datetime.now()\n        print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n\n        # Training phase\n        model.train()\n        train_loss, correct_preds, total_samples = 0.0, 0, 0\n\n        for inputs, labels in train_loader:\n            labels = labels - 1  \n            inputs, labels = inputs.to(device), labels.to(device)\n\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n            train_loss += loss.item() * inputs.size(0)\n            _, predictions = torch.max(outputs, 1)\n            correct_preds += (predictions == labels).sum().item()\n            total_samples += labels.size(0)\n\n        avg_train_loss = train_loss / total_samples\n        train_accuracy = correct_preds / total_samples\n        history[\"train\"][\"losses\"].append(avg_train_loss)\n        history[\"train\"][\"accuracies\"].append(train_accuracy * 100)\n\n        # Validation phase\n        model.eval()\n        val_loss, correct_preds, total_samples = 0.0, 0, 0\n\n        with torch.no_grad():\n            for inputs, labels in val_loader:\n                labels = labels - 1  \n                inputs, labels = inputs.to(device), labels.to(device)\n                outputs = model(inputs)\n                loss = criterion(outputs, labels)\n\n                val_loss += loss.item() * inputs.size(0)\n                _, predictions = torch.max(outputs, 1)\n                correct_preds += (predictions == labels).sum().item()\n                total_samples += labels.size(0)\n\n        avg_val_loss = val_loss / total_samples\n        val_accuracy = correct_preds / total_samples\n        history[\"val\"][\"losses\"].append(avg_val_loss)\n        history[\"val\"][\"accuracies\"].append(val_accuracy * 100)\n\n        epoch_duration = (datetime.now() - epoch_start_time).total_seconds() / 60  # Minutes\n        history[\"train\"][\"epoch_times\"].append(epoch_duration)\n\n        print(f\"Train Loss: {avg_train_loss:.4f}, Train Acc: {train_accuracy:.4f}\")\n        print(f\"Val Loss: {avg_val_loss:.4f}, Val Acc: {val_accuracy:.4f}\")\n        print(f\"Epoch duration: {epoch_duration:.2f} min\")\n\n        # Save best model based on validation accuracy\n        if val_accuracy > best_val_acc:\n            best_val_acc = val_accuracy\n            torch.save(model.state_dict(), model_save_path)\n            print(f\"Best model saved with Val Acc: {val_accuracy:.4f}\")\n\n    total_training_duration = (datetime.now() - total_training_start).total_seconds() / 60 \n    history[\"total_training_time\"] = total_training_duration\n    print(f\"\\nTotal training time: {total_training_duration:.2f} min\")\n\n    return history","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T21:29:29.304410Z","iopub.execute_input":"2025-04-14T21:29:29.304717Z","iopub.status.idle":"2025-04-14T21:29:29.315285Z","shell.execute_reply.started":"2025-04-14T21:29:29.304695Z","shell.execute_reply":"2025-04-14T21:29:29.314540Z"}},"outputs":[],"execution_count":6},{"id":"27c4a915-f9d0-40b9-aa5d-d84a0ec501ca","cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T21:29:35.474200Z","iopub.execute_input":"2025-04-14T21:29:35.474536Z","iopub.status.idle":"2025-04-14T21:29:35.527359Z","shell.execute_reply.started":"2025-04-14T21:29:35.474505Z","shell.execute_reply":"2025-04-14T21:29:35.526495Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":7},{"id":"18957d75","cell_type":"code","source":"#initialize model \nmodel = Resnet18(num_of_classes=26)\n#initialize weights using He (Kaiming) uniform initialization\nfor module in model.modules():\n    if isinstance(module, (nn.Conv2d, nn.Linear)):\n        nn.init.kaiming_uniform_(module.weight, nonlinearity=\"relu\")\n\n#define loss criterion and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n#training the model on EMNIST Letters dataset\nresnet_he_acc_state = train_model(\n    model=model,\n    train_loader=train_loader,\n    val_loader=val_loader,\n    criterion=criterion,\n    optimizer=optimizer,\n    num_epochs=10,\n    device=device,\n    model_save_path=\"resnet_emnist_letters.pth\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T21:29:37.858701Z","iopub.execute_input":"2025-04-14T21:29:37.859027Z","iopub.status.idle":"2025-04-14T22:05:25.365424Z","shell.execute_reply.started":"2025-04-14T21:29:37.858985Z","shell.execute_reply":"2025-04-14T22:05:25.364513Z"}},"outputs":[{"name":"stdout","text":"\nEpoch 1/10\nTrain Loss: 0.3352, Train Acc: 0.8911\nVal Loss: 0.2570, Val Acc: 0.9172\nEpoch duration: 3.61 min\nBest model saved with Val Acc: 0.9172\n\nEpoch 2/10\nTrain Loss: 0.1982, Train Acc: 0.9321\nVal Loss: 0.1859, Val Acc: 0.9372\nEpoch duration: 3.58 min\nBest model saved with Val Acc: 0.9372\n\nEpoch 3/10\nTrain Loss: 0.1657, Train Acc: 0.9421\nVal Loss: 0.1850, Val Acc: 0.9377\nEpoch duration: 3.57 min\nBest model saved with Val Acc: 0.9377\n\nEpoch 4/10\nTrain Loss: 0.1484, Train Acc: 0.9461\nVal Loss: 0.1780, Val Acc: 0.9378\nEpoch duration: 3.58 min\nBest model saved with Val Acc: 0.9378\n\nEpoch 5/10\nTrain Loss: 0.1292, Train Acc: 0.9518\nVal Loss: 0.1659, Val Acc: 0.9455\nEpoch duration: 3.57 min\nBest model saved with Val Acc: 0.9455\n\nEpoch 6/10\nTrain Loss: 0.1147, Train Acc: 0.9562\nVal Loss: 0.1509, Val Acc: 0.9501\nEpoch duration: 3.58 min\nBest model saved with Val Acc: 0.9501\n\nEpoch 7/10\nTrain Loss: 0.1026, Train Acc: 0.9607\nVal Loss: 0.1706, Val Acc: 0.9462\nEpoch duration: 3.58 min\n\nEpoch 8/10\nTrain Loss: 0.0920, Train Acc: 0.9638\nVal Loss: 0.1632, Val Acc: 0.9472\nEpoch duration: 3.57 min\n\nEpoch 9/10\nTrain Loss: 0.0808, Train Acc: 0.9675\nVal Loss: 0.1771, Val Acc: 0.9471\nEpoch duration: 3.58 min\n\nEpoch 10/10\nTrain Loss: 0.0712, Train Acc: 0.9708\nVal Loss: 0.1949, Val Acc: 0.9436\nEpoch duration: 3.57 min\n\nTotal training time: 35.79 min\n","output_type":"stream"}],"execution_count":8},{"id":"4d66641e","cell_type":"code","source":"test_emnist = datasets.EMNIST(root='./data', split='letters', train=False, download=True, transform=transform)\n\ntest_size = int(0.3 * len(test_emnist))\nunused_size = len(test_emnist) - test_size\ntest_dataset, _ = torch.utils.data.random_split(test_emnist, [test_size, unused_size])\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)\ncriterion = nn.CrossEntropyLoss()\n\n# Evaluation function\ndef evaluate(model, weight_path, device):\n    model.load_state_dict(torch.load(weight_path))\n    model.to(device)\n    model.eval()\n\n    # Adjust for EMNIST Letters (26 classes)\n    accuracy_metric = Accuracy(task=\"multiclass\", num_classes=26).to(device)\n\n    performances = {\n        \"test\": {\n            \"loader\": test_loader,\n        }\n    }\n\n    for dataset_name in performances:\n        all_labels = []\n        all_preds = []\n        all_probabilities = []\n        total_loss = 0.0\n\n        with torch.no_grad():\n            for inputs, labels in performances[dataset_name][\"loader\"]:\n                labels = labels - 1\n                inputs, labels = inputs.to(device), labels.to(device)\n                outputs = model(inputs)\n                loss = criterion(outputs, labels)\n\n                total_loss += loss.item()\n                probabilities = torch.softmax(outputs, dim=1)\n                _, predicted = torch.max(outputs.data, 1)\n\n                all_labels.extend(labels.cpu().numpy())\n                all_preds.extend(predicted.cpu().numpy())\n                all_probabilities.extend(probabilities.cpu().numpy())\n\n        avg_loss = total_loss / len(performances[dataset_name][\"loader\"])\n        acc = accuracy_metric(torch.tensor(all_preds), torch.tensor(all_labels)) * 100\n\n        performances[dataset_name].update({\n            \"loss\": avg_loss,\n            \"accuracy\": acc.item(),\n            \"all_labels\": all_labels,\n            \"all_preds\": all_preds,\n            \"all_probabilities\": all_probabilities\n        })\n\n    return performances","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T22:07:20.332102Z","iopub.execute_input":"2025-04-14T22:07:20.332540Z","iopub.status.idle":"2025-04-14T22:07:20.366867Z","shell.execute_reply.started":"2025-04-14T22:07:20.332501Z","shell.execute_reply":"2025-04-14T22:07:20.365983Z"}},"outputs":[],"execution_count":9},{"id":"dba61c53","cell_type":"code","source":"# Evaluate the model\nperformance = evaluate(Resnet18(), \"resnet_emnist_letters.pth\", device = device)\nprint(\"Test accuracy:\", performance[\"test\"][\"accuracy\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T22:07:25.121421Z","iopub.execute_input":"2025-04-14T22:07:25.121762Z","iopub.status.idle":"2025-04-14T22:07:30.843690Z","shell.execute_reply.started":"2025-04-14T22:07:25.121734Z","shell.execute_reply":"2025-04-14T22:07:30.842860Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-9-a62e0335fe21>:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(weight_path))\n","output_type":"stream"},{"name":"stdout","text":"Test accuracy: 95.12820434570312\n","output_type":"stream"}],"execution_count":10},{"id":"2c6a1adc","cell_type":"code","source":"model = Resnet18(num_of_classes=26)  # Set to 26 for EMNIST letters\nmodel.load_state_dict(torch.load(\"resnet_emnist_letters.pth\", map_location=torch.device(\"cpu\")))\ntorch.save(model.state_dict(), \"resnet_emnist_letters_cpu.pth\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T22:07:38.012671Z","iopub.execute_input":"2025-04-14T22:07:38.012947Z","iopub.status.idle":"2025-04-14T22:07:38.183198Z","shell.execute_reply.started":"2025-04-14T22:07:38.012928Z","shell.execute_reply":"2025-04-14T22:07:38.182255Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-11-807bb6a2a303>:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(\"resnet_emnist_letters.pth\", map_location=torch.device(\"cpu\")))\n","output_type":"stream"}],"execution_count":11}]}